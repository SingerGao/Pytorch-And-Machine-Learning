{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建了模型之后，我们需要面临一个问题：如何制定一个合理的规则找到模型的最优参数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 重新看数据标签与模型的输出（离散概率分布）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将标签one-hot向量看成真实概率分布p(x)\n",
    "\n",
    "  ![label_distribution](./label_distribution.png)\n",
    "\n",
    "  \n",
    "\n",
    "- 将模型预测结果看成近似分布q(x)\n",
    "\n",
    "  ![Model_distribution](./Model_distribution.png)\n",
    "\n",
    "**问题**：如何让模型预测出的分布接近真实分布？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 信息\n",
    "\n",
    "#### 2.1.1 什么是信息？\n",
    "\n",
    "- 信息的***定义***\n",
    "\n",
    "  一个事件发生的***惊喜度***。\n",
    "\n",
    "- 信息的***特性***\n",
    "\n",
    "  - 信息关于概率***单调递减***：概率越小的事件发生，信息量越大；概率越大的事件发生，信息量越小。\n",
    "\n",
    "  - 信息具有***独立性***：以h(.)表示信息函数，若事件x与y独立，会有以下关系\n",
    "    $$\n",
    "    h(x,y)=h(x)+h(y)\n",
    "    $$\n",
    "\n",
    "#### 2.1.2 怎么度量信息？\n",
    "\n",
    "　　通过事件发生的概率来度量信息，可以设计信息函数：\n",
    "\n",
    "![info_prop](./info_prop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 信息熵\n",
    "\n",
    "- 信息熵的***定义***\n",
    "\n",
    "  平均信息/信息的期望：\n",
    "  $$\n",
    "  \\begin{align}\n",
    "  H(x)&=\\sum_{i=1}^{n} p(x_i)[-log_2 p(x_i)](信息的加权平均)\\\\\n",
    "  &=E_{x_i\\sim p(x_i)}[-log_2 p(x_i)](信息的期望)\n",
    "  \\end{align}\n",
    "  $$\n",
    "\n",
    "- 信息熵的***功能***\n",
    "\n",
    "  信息熵可以***描述不确定性***：\n",
    "\n",
    "  - \n",
    "  ![entropy1](./entropy1.png)\n",
    "  - \n",
    "  ![Entropy2](./Entropy2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 KL散度(相关熵)\n",
    "\n",
    "在机器学习里经常需要面对一个问题：如何度量真实分布与近似分布的距离？\n",
    "\n",
    "可以用KL散度(相关熵)度量两个分布的距离。\n",
    "\n",
    "- KL散度的**定义**\n",
    "  $$\n",
    "  \\begin{align}\n",
    "  KL(p||q)=-\\sum_{i=1}^{n} p(x_i)\\log(\\frac{q(x_i)}{p(x_i)})\n",
    "  \\end{align}\n",
    "  $$\n",
    "\n",
    "- KL散度的**性质**\n",
    "\n",
    "  - 非对称:\n",
    "    $\n",
    "    KL(p||q)\\not\\equiv KL(q||p)\n",
    "    $\n",
    "\n",
    "  - 非负(只有两个分布完全相等的时候为0):\n",
    "    $\n",
    "    KL(p||q)\\geq0\n",
    "    $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 交叉熵(简化了KL散度)\n",
    "\n",
    "　　KL散度是近似分布q(x)与真实分布p(x)的距离。\n",
    "$$\n",
    "\\begin{align}\n",
    "KL(p||q)=-\\sum_{i=1}^{n} p(x_i)\\log(q(x_i))-&\\underline{[-\\sum_{i=1}^{n} p(x_i) \\log(p(x_i))]}\n",
    "\\\\& 真实分布的信息熵(常量)\n",
    "\\end{align}\n",
    "$$\n",
    "因为第二项是常量，所以可以不考虑，保留第一项即是交叉熵：\n",
    "$$\n",
    "\\begin{align}\n",
    "CE=-\\sum_{i=1}^{n} p(x_i)\\log(q(x_i))\n",
    "\\end{align}\n",
    "$$\n",
    "　　因此，也可以简单地将交叉熵看成是近似分布q(x)与真实分布p(x)的距离，距离越小两个分布越相似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 最优化问题——最小化交叉熵损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　图片对应的标签是真实概率分布p(x)，模型预测图片的结果是近似概率分布q(x)，如果它们的交叉熵小，则说明近似的结果很接近真实。因此，我们可以借助交叉熵设计一个规则来找模型的最优参数，即找到交叉熵最小的模型参数。\n",
    "\n",
    "　假设一个实例$x$对应的标签为$y$(看成真实分布$p(x)$)；分类器模型为$f_\\theta(x)$，$f_\\theta^i(x)$表示分类结果的第$i$维。通过交叉熵损失找模型最优参数$\\theta^*$的过程可以被表述为一个最优化问题(即交叉熵损失函数的具体形式)：\n",
    "$$\n",
    "\\min_{\\theta} -\\sum_{i=1}^{n} y^i\\log(f_\\theta^i(x))\n",
    "$$\n",
    "注意交叉熵是一个标量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 .Pytorch中的交叉熵损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　　Pytorch带有交叉熵损失函数torch.nn.CrossEntropyLoss()，它做了一些改变：\n",
    "\n",
    "- 去掉求和符号\n",
    "\n",
    "  　　因为$y$为one-hot向量，所以只有维度为１的那一项不为0，其它项为0。实际上可以将交叉熵损失写成一项：\n",
    "  $$\n",
    "  loss(f_\\theta(x), target)=-log(f_\\theta^{target}(x))\n",
    "  $$\n",
    "  其中target为标量标签，而不是one-hot标签。\n",
    "\n",
    "- 合并Softmax\n",
    "\n",
    "     torch.nn.CrossEntropyLoss()合并了softmax，因此模型的最后一层可以不用是softmax：\n",
    "  $$\n",
    "  loss(f_\\theta(x), target)=-log(\\frac{\\exp f_\\theta^{target}(x)}{\\sum_i \\exp f_\\theta^{i}(x)})\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch中使用交叉熵损失函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3447)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "loss=nn.CrossEntropyLoss()\n",
    "input=torch.Tensor([[-0.7715, -0.6205,-0.2562]])\n",
    "target = torch.tensor([0])\n",
    "output = loss(input, target)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
